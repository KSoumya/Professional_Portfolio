{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import requests\n",
    "import os \n",
    "import pydata_google_auth\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import language\n",
    "import json \n",
    "from google.cloud import storage \n",
    "from google.cloud import bigquery\n",
    "\n",
    "def get_today_news():\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]='<GOOGLE_CRED>'\n",
    "    auth = requests.auth.HTTPBasicAuth('CLIENT_ID', 'SECRET_TOKEN')\n",
    "    data = {\n",
    "    'grant_type': 'client_credentials',\n",
    "    'username': 'username',\n",
    "    'password': 'password'\n",
    "    }\n",
    "    headers = {'User-Agent': 'News/0.0.1'}\n",
    "    request = requests.post('https://www.reddit.com/api/v1/access_token', auth=auth, data=data, headers=headers)\n",
    "    token = request.json()['access_token']\n",
    "    headers = {**headers, **{'Authorization': f\"bearer {token}\"}}\n",
    "    \n",
    "    news_requests = requests.get('https://oauth.reddit.com/r/news/hot', headers=headers, params={'limit': '100'})\n",
    "    df = pd.DataFrame()\n",
    "    for post in news_requests.json()['data']['children']:\n",
    "        df = df.append({\n",
    "        'title': post['data']['title'],\n",
    "        'upvote_ratio': post['data']['upvote_ratio'],\n",
    "        'score': post['data']['score'],\n",
    "        'ups': post['data']['ups'],\n",
    "        'domain': post['data']['domain'],\n",
    "        'num_comments': post['data']['num_comments']\n",
    "    }, ignore_index=True)\n",
    "    \n",
    "    client = bigquery.Client()\n",
    "    news_dataset_id = 'reddit_news'\n",
    "    news_table_id = 'r_news'\n",
    "    \n",
    "    news_ref = client.dataset(news_dataset_id)\n",
    "    news_table_id = news_ref.table(news_table_id)\n",
    "    \n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.write_disposition='WRITE_TRUNCATE'\n",
    "    job_config.source_format = bigquery.SourceFormat.CSV\n",
    "    job_config.autodetect=True\n",
    "    job_config.ignore_unknown_values=True \n",
    "\n",
    "    job = client.load_table_from_dataframe(\n",
    "    df,\n",
    "    news_table_id,\n",
    "    location='US',\n",
    "    job_config=job_config)\n",
    "    \n",
    "    job.result()\n",
    "    print('News table loaded.')\n",
    "    \n",
    "    not_onion_requests = requests.get('https://oauth.reddit.com/r/nottheonion/hot', headers=headers, params={'limit': '100'})\n",
    "    not_onion_df = pd.DataFrame()\n",
    "    for not_onion in not_onion_requests.json()['data']['children']:\n",
    "        not_onion_df = not_onion_df.append(\n",
    "        {\n",
    "            'domain': not_onion['data']['domain'],\n",
    "            'num_comments': not_onion['data']['num_comments'],\n",
    "            'score': not_onion['data']['score'],\n",
    "            'title': not_onion['data']['title'],\n",
    "            'ups': not_onion['data']['ups'],\n",
    "            'upvote_ratio': not_onion['data']['upvote_ratio']\n",
    "        }, ignore_index=True)\n",
    "    \n",
    "    client = bigquery.Client()\n",
    "    onion_dataset_id = 'reddit_news'\n",
    "    onion_table_id = 'not_the_onion'\n",
    "    \n",
    "    onion_ref = client.dataset(onion_dataset_id)\n",
    "    onion_table_id = onion_ref.table(onion_table_id)\n",
    "    \n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.write_disposition='WRITE_TRUNCATE'\n",
    "    job_config.source_format = bigquery.SourceFormat.CSV\n",
    "    job_config.autodetect=True\n",
    "    job_config.ignore_unknown_values=True \n",
    "\n",
    "    job = client.load_table_from_dataframe(\n",
    "    not_onion_df,\n",
    "    onion_table_id,\n",
    "    location='US',\n",
    "    job_config=job_config)\n",
    "    \n",
    "    job.result()\n",
    "    print('Not the onion table loaded.')\n",
    "    \n",
    "    offbeat_request = requests.get('https://oauth.reddit.com/r/offbeat/hot', headers=headers, params={'limit': '100'})\n",
    "    offbeat_df = pd.DataFrame()\n",
    "    for offbeat in offbeat_request.json()['data']['children']:\n",
    "        offbeat_df = offbeat_df.append({\n",
    "            'domain': offbeat['data']['domain'],\n",
    "            'num_comments': offbeat['data']['num_comments'],\n",
    "            'score': offbeat['data']['score'],\n",
    "            'title': offbeat['data']['title'],\n",
    "            'ups': offbeat['data']['ups'],\n",
    "            'upvote_ratio': offbeat['data']['upvote_ratio']\n",
    "        }, ignore_index=True)\n",
    "    \n",
    "    client = bigquery.Client()\n",
    "    offbeat_dataset_id = 'reddit_news'\n",
    "    offbeat_table_id = 'offbeat'\n",
    "    \n",
    "    offbeat_ref = client.dataset(offbeat_dataset_id)\n",
    "    offbeat_table_id = offbeat_ref.table(offbeat_table_id)\n",
    "    \n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.write_disposition='WRITE_TRUNCATE'\n",
    "    job_config.source_format = bigquery.SourceFormat.CSV\n",
    "    job_config.autodetect=True\n",
    "    job_config.ignore_unknown_values=True \n",
    "\n",
    "    job = client.load_table_from_dataframe(\n",
    "    offbeat_df,\n",
    "    offbeat_table_id,\n",
    "    location='US',\n",
    "    job_config=job_config)\n",
    "    \n",
    "    job.result()\n",
    "    print('Offbeat table loaded.')\n",
    "    \n",
    "    the_news_request = requests.get('https://oauth.reddit.com/r/thenews/hot', headers=headers, params={'limit': '100'})\n",
    "    the_news_df = pd.DataFrame()\n",
    "    for news in the_news_request.json()['data']['children']:\n",
    "        the_news_df = the_news_df.append({\n",
    "        'domain': news['data']['domain'],\n",
    "        'num_comments': news['data']['num_comments'],\n",
    "        'score': news['data']['score'],\n",
    "        'title': news['data']['title'],\n",
    "        'ups': news['data']['ups'],\n",
    "        'upvote_ratio': news['data']['upvote_ratio']\n",
    "        }, ignore_index=True)\n",
    "    \n",
    "    client = bigquery.Client()\n",
    "    the_news_dataset_id = 'reddit_news'\n",
    "    the_news_table_id = 'the_news'\n",
    "    \n",
    "    the_news_ref = client.dataset(the_news_dataset_id)\n",
    "    the_news_table_id = the_news_ref.table(the_news_table_id)\n",
    "    \n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.write_disposition='WRITE_TRUNCATE'\n",
    "    job_config.source_format = bigquery.SourceFormat.CSV\n",
    "    job_config.autodetect=True\n",
    "    job_config.ignore_unknown_values=True \n",
    "\n",
    "    job = client.load_table_from_dataframe(\n",
    "    the_news_df,\n",
    "    the_news_table_id,\n",
    "    location='US',\n",
    "    job_config=job_config)\n",
    "    \n",
    "    job.result()\n",
    "    print('The news table loaded.')\n",
    "    \n",
    "    us_news_request = requests.get('https://oauth.reddit.com/r/USNews/hot', headers=headers, params={'limit': '100'})\n",
    "    us_news_df = pd.DataFrame()\n",
    "    for us_news in us_news_request.json()['data']['children']:\n",
    "        us_news_df = us_news_df.append({\n",
    "            'domain': us_news['data']['domain'],\n",
    "            'num_comments': us_news['data']['num_comments'],\n",
    "            'score': us_news['data']['score'],\n",
    "            'title': us_news['data']['title'],\n",
    "            'ups': us_news['data']['ups'],\n",
    "            'upvote_ratio': us_news['data']['upvote_ratio']\n",
    "        }, ignore_index=True)\n",
    "    \n",
    "    client = bigquery.Client()\n",
    "    us_news_dataset_id = 'reddit_news'\n",
    "    us_news_table_id = 'us_news'\n",
    "    \n",
    "    us_news_ref = client.dataset(us_news_dataset_id)\n",
    "    us_news_table_id = us_news_ref.table(us_news_table_id)\n",
    "    \n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.write_disposition='WRITE_TRUNCATE'\n",
    "    job_config.source_format = bigquery.SourceFormat.CSV\n",
    "    job_config.autodetect=True\n",
    "    job_config.ignore_unknown_values=True \n",
    "\n",
    "    job = client.load_table_from_dataframe(\n",
    "    us_news_df,\n",
    "    us_news_table_id,\n",
    "    location='US',\n",
    "    job_config=job_config)\n",
    "    \n",
    "    job.result()\n",
    "    print('U.S. news table loaded.')\n",
    "    \n",
    "    full_news_request = requests.get('https://oauth.reddit.com/r/Full_news/hot', headers=headers, params={'limit': '100'})\n",
    "    full_news_df = pd.DataFrame()\n",
    "    for full_news in full_news_request.json()['data']['children']:\n",
    "        full_news_df = full_news_df.append({\n",
    "            'domain': full_news['data']['domain'],\n",
    "            'num_comments': full_news['data']['num_comments'],\n",
    "            'score': full_news['data']['score'],\n",
    "            'title': full_news['data']['title'],\n",
    "            'ups': full_news['data']['ups'],\n",
    "            'upvote_ratio': full_news['data']['upvote_ratio']\n",
    "        }, ignore_index=True)\n",
    "    \n",
    "    client = bigquery.Client()\n",
    "    full_news_dataset_id = 'reddit_news'\n",
    "    full_news_table_id = 'full_news'\n",
    "    \n",
    "    full_news_ref = client.dataset(full_news_dataset_id)\n",
    "    full_news_table_id = full_news_ref.table(full_news_table_id)\n",
    "    \n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.write_disposition='WRITE_TRUNCATE'\n",
    "    job_config.source_format = bigquery.SourceFormat.CSV\n",
    "    job_config.autodetect=True\n",
    "    job_config.ignore_unknown_values=True \n",
    "\n",
    "    job = client.load_table_from_dataframe(\n",
    "    full_news_df,\n",
    "    full_news_table_id,\n",
    "    location='US',\n",
    "    job_config=job_config)\n",
    "    \n",
    "    job.result()\n",
    "    print('Full news table loaded.')\n",
    "    \n",
    "    quality_news_request = requests.get('https://oauth.reddit.com/r/qualitynews/hot', headers=headers, params={'limit': '100'})\n",
    "    quality_news_df = pd.DataFrame()\n",
    "    for quality in quality_news_request.json()['data']['children']:\n",
    "        quality_news_df = quality_news_df.append({\n",
    "            'domain': quality['data']['domain'],\n",
    "            'num_comments': quality['data']['num_comments'],\n",
    "            'score': quality['data']['score'],\n",
    "            'title': quality['data']['title'],\n",
    "            'ups': quality['data']['ups'],\n",
    "            'upvote_ratio': quality['data']['upvote_ratio']\n",
    "        }, ignore_index=True)\n",
    "    \n",
    "    client = bigquery.Client()\n",
    "    quality_news_dataset_id = 'reddit_news'\n",
    "    quality_news_table_id = 'quality_news'\n",
    "    \n",
    "    quality_news_ref = client.dataset(quality_news_dataset_id)\n",
    "    quality_news_table_id = quality_news_ref.table(quality_news_table_id)\n",
    "    \n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.write_disposition='WRITE_TRUNCATE'\n",
    "    job_config.source_format = bigquery.SourceFormat.CSV\n",
    "    job_config.autodetect=True\n",
    "    job_config.ignore_unknown_values=True \n",
    "\n",
    "    job = client.load_table_from_dataframe(\n",
    "    quality_news_df,\n",
    "    quality_news_table_id,\n",
    "    location='US',\n",
    "    job_config=job_config)\n",
    "    \n",
    "    job.result()\n",
    "    print('Quality news table loaded.')\n",
    "    \n",
    "    uplifting_news_request = requests.get('https://oauth.reddit.com/r/upliftingnews', headers=headers, params={'limit': '100'})\n",
    "    uplifting_news_df = pd.DataFrame()\n",
    "    for uplifting in uplifting_news_request.json()['data']['children']:\n",
    "        uplifting_news_df = uplifting_news_df.append({\n",
    "            'domain': uplifting['data']['domain'],\n",
    "            'num_comments': uplifting['data']['num_comments'],\n",
    "            'score': uplifting['data']['score'],\n",
    "            'title': uplifting['data']['title'],\n",
    "            'ups': uplifting['data']['ups'],\n",
    "            'upvote_ratio': uplifting['data']['upvote_ratio']\n",
    "        }, ignore_index=True)\n",
    "    \n",
    "    client = bigquery.Client()\n",
    "    uplifting_news_dataset_id = 'reddit_news'\n",
    "    uplifting_news_table_id = 'uplifting_news'\n",
    "    \n",
    "    uplifting_news_ref = client.dataset(uplifting_news_dataset_id)\n",
    "    uplifting_news_table_id = uplifting_news_ref.table(uplifting_news_table_id)\n",
    "    \n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.write_disposition='WRITE_TRUNCATE'\n",
    "    job_config.source_format = bigquery.SourceFormat.CSV\n",
    "    job_config.autodetect=True\n",
    "    job_config.ignore_unknown_values=True \n",
    "\n",
    "    job = client.load_table_from_dataframe(\n",
    "    uplifting_news_df,\n",
    "    uplifting_news_table_id,\n",
    "    location='US',\n",
    "    job_config=job_config)\n",
    "    \n",
    "    job.result()\n",
    "    print('Uplifting news table loaded.')\n",
    "    \n",
    "    in_the_news_request = requests.get('https://oauth.reddit.com/r/inthenews', headers=headers, params={'limit': '100'})\n",
    "    in_the_news_df = pd.DataFrame()\n",
    "    for in_news in in_the_news_request.json()['data']['children']:\n",
    "        in_the_news_df = in_the_news_df.append({\n",
    "            'domain': in_news['data']['domain'],\n",
    "            'num_comments': in_news['data']['num_comments'],\n",
    "            'score': in_news['data']['score'],\n",
    "            'title': in_news['data']['title'],\n",
    "            'ups': in_news['data']['ups'],\n",
    "            'upvote_ratio': in_news['data']['upvote_ratio']\n",
    "        }, ignore_index=True)\n",
    "        \n",
    "    client = bigquery.Client()\n",
    "    in_the_news_dataset_id = 'reddit_news'\n",
    "    in_the_news_table_id = 'in_the_news'\n",
    "    \n",
    "    in_the_news_ref = client.dataset(in_the_news_dataset_id)\n",
    "    in_the_news_table_id = in_the_news_ref.table(in_the_news_table_id)\n",
    "    \n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.write_disposition='WRITE_TRUNCATE'\n",
    "    job_config.source_format = bigquery.SourceFormat.CSV\n",
    "    job_config.autodetect=True\n",
    "    job_config.ignore_unknown_values=True \n",
    "\n",
    "    job = client.load_table_from_dataframe(\n",
    "    in_the_news_df,\n",
    "    in_the_news_table_id,\n",
    "    location='US',\n",
    "    job_config=job_config)\n",
    "    \n",
    "    job.result()\n",
    "    print('In the news table loaded.')\n",
    "    \n",
    "    return print('News for today has loaded.')\n",
    "    \n",
    "get_today_news()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
